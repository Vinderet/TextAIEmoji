import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# ...

def preprocess_text(text):
    # Приведение к нижнему регистру
    text = text.lower()
    
    # Удаление специальных символов
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    
    # Токенизация
    words = word_tokenize(text)
    
    # Удаление стоп-слов
    stop_words = set(stopwords.words('русский'))  # Подставьте свой язык
    filtered_words = [word for word in words if word.lower() not in stop_words]
    
    # Объединение токенов обратно в текст
    filtered_text = ' '.join(filtered_words)
    
    return filtered_text

# Применение предварительной обработки перед токенизацией
X_train = X_train.apply(preprocess_text)
X_test = X_test.apply(preprocess_text)

# Затем продолжайте с токенизацией и обучением модели
